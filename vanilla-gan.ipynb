{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN\n",
    "![image about general adversarial networks (GAN)](./resources/gan.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Discriminator\n",
    "- using the MNIST dataset, a single image is going to be 28x28x1\n",
    "- a single color channel (grayscale) 28x28 images\n",
    "<br/> <br/><br/>\n",
    "![convolutional layer output formula](./resources/convolutional-layer-formula.png)\n",
    "- formula for calculating the output dimension of the convolution operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 26, 26]             640\n",
      "              ReLU-2           [-1, 64, 26, 26]               0\n",
      "            Conv2d-3          [-1, 128, 24, 24]          73,856\n",
      "              ReLU-4          [-1, 128, 24, 24]               0\n",
      "            Conv2d-5          [-1, 256, 22, 22]         295,168\n",
      "              ReLU-6          [-1, 256, 22, 22]               0\n",
      "            Conv2d-7          [-1, 256, 20, 20]         590,080\n",
      "              ReLU-8          [-1, 256, 20, 20]               0\n",
      "           Flatten-9               [-1, 102400]               0\n",
      "           Linear-10                    [-1, 1]         102,401\n",
      "================================================================\n",
      "Total params: 1,062,145\n",
      "Trainable params: 1,062,145\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 6.02\n",
      "Params size (MB): 4.05\n",
      "Estimated Total Size (MB): 10.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel_sz = (3, 3)\n",
    "        self.stride = 1\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # 1x28x28\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=self.kernel_sz, stride=self.stride), # 64x26x26\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=self.kernel_sz, stride=self.stride), # 128x24x24\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=self.kernel_sz, stride=self.stride), # 256x22x22\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=self.kernel_sz, stride=self.stride), # 256x20x20\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=256 * 20 * 20, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "Discriminator = Discriminator()\n",
    "Discriminator = Discriminator.to(device) # ensuring the whole nn is in a single device\n",
    "summary(model=Discriminator, input_size=(1, 28, 28), device=str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Generator\n",
    "![](./resources/transposed-convolutional-layer-formula.png)\n",
    "- formula for calculating the output dimension of the transposed convolution operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 3136]         316,736\n",
      "              ReLU-2                 [-1, 3136]               0\n",
      "         Unflatten-3             [-1, 64, 7, 7]               0\n",
      "   ConvTranspose2d-4           [-1, 32, 16, 16]          32,800\n",
      "              ReLU-5           [-1, 32, 16, 16]               0\n",
      "   ConvTranspose2d-6           [-1, 16, 34, 34]           8,208\n",
      "              ReLU-7           [-1, 16, 34, 34]               0\n",
      "            Conv2d-8            [-1, 1, 28, 28]             785\n",
      "              Tanh-9            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 358,529\n",
      "Trainable params: 358,529\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.49\n",
      "Params size (MB): 1.37\n",
      "Estimated Total Size (MB): 1.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_dim, out_features=7*7*64),  # [n, 256, 7, 7]\n",
    "            nn.ReLU(),\n",
    "            # reshape data from 1 dimension to batch of 2 dimensional data\n",
    "            nn.Unflatten(dim=1, unflattened_size=(64, 7, 7)),\n",
    "            \n",
    "            # reduce feature maps to 32, increase image size to 16x16\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # reduce feature maps to 16, increase image size to 28x28\n",
    "            nn.ConvTranspose2d(in_channels=32, out_channels=16, kernel_size=4, stride=2), # [n, 16, 34, 34]\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # reduce feature maps to 1, decrease image size to 28x28 \n",
    "            nn.Conv2d(in_channels=16, out_channels=1, kernel_size=7),  # [n, 1, 28, 28]\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# assume latent_dim for now\n",
    "latent_dim = 100\n",
    "Generator = Generator(latent_dim=latent_dim)\n",
    "Generator = Generator.to(device=device)\n",
    "summary(model=Generator, input_size=(latent_dim,), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784  # 28x28 pixels\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10  # Reduced for demonstration purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"dataset/\", download=True, train=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root=\"dataset/\", download=True, train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(10, 3))\n",
    "for i in range(6):\n",
    "    axes[i].imshow(example_data[i][0], cmap=\"gray\")\n",
    "    axes[i].set_title(f\"Label: {example_targets[i]}\")\n",
    "    axes[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim of single image = torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"dim of single image = {example_data[0].size()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
